---
title: "Exercise-2"
author: "Riyanshi Bohra"
format: html
editor: visual
---

Through this project, our goal is to practice basic R commands/ methods for descriptive data analysis.

# Setting up

```{r}
# Installing and loading required packages
if(!require('pacman'))
  install.packages("pacman")

library(pacman)
p_load(DBI, # DBI databases
       dlookr,
       here, # Reproducible/ standard directories
       janitor,
       RMySQL, # Utilizing MySQL drivers
       tidymodels, # Tidyverse format modeling (e.g., lm())
       tidyverse, # Data wrangling, manipulation, visualization
       qqplotr) 
```

# Loading the data

```{r}
# Loading the x.csv file
data <- read_csv(here("data", "x.csv"))

data |> glimpse()
```

```{r}
data <- read_delim(here("data", "x.tsv"))

data |> glimpse()
```

```{r}
#Importing data from MySQL database
# NOTE: I do not have the local MySQL database 'etcsite_charaparser' set up on my machine.
# Therefore, I am skipping the database connection and query part in this code.
```

# Data Cleaning

## Wide vs. Long format data

This code is dealing with the concepts of "wide" and "long" data formats, two common ways to structure data in a dataframe or table.

```{r}
# Reading the wide.txt file which is in wide format

wide <- read_delim(here("data", "wide.txt"), delim = " ", skip = 1, col_names = c("Name", "Math", "English", "Degree_Year"))
```

```{r}
# Converts wide dataframe to a long format
long <- wide |>
  pivot_longer(cols = c(Math, English),
               names_to = "Subject", 
               values_to = "Grade")
long
```

```{r}
# Converts the long dataframe back to a wide format
wide <- long %>%
  pivot_wider(names_from = Subject, values_from = Grade)
wide
```

## Split one column to multiple columns

```{r}
# Split Degree_Year to two separate columns Degree and Year
clean <- long %>%
  separate(Degree_Year, c("Degree", "Year"), sep = "_")

clean
```

## Handling date/time and time zones format

```{r}
# Installing and loading necessary libraries 
if(!require('lubridate'))
  install.packages("lubridate")
library(lubridate)
```

```{r}
#Converting dates of varying format to one standard format(year-month-day)

mixed.dates <- c(20140123, "2019-12-12", "2009/5/1",
 "measured on 2002-12-06", "2018-7/16")
clean.dates <- ymd(mixed.dates) #convert to year-month-day 
clean.dates
```

```{r}
# Extracting day,week,month,year from date section for easier analysis

data.frame(Dates = clean.dates, WeekDay = wday(clean.dates), nWeekDay = wday(clean.dates, label = TRUE), Year = year(clean.dates), Month = month(clean.dates, label = TRUE))
```

```{r}
# Dealing with Time Zones

date.time <- ymd_hms("20190203 03:00:03", tz="Asia/Shanghai")
```

```{r}
# Converting all time zones to Phoenix, Arizona Time Zone

with_tz(date.time, tz="America/Phoenix")
```

```{r}
# Changing the timezone for a time

force_tz(date.time, "Turkey")
```

```{r}
# Checking all the available time zones-

OlsonNames()
```

## String Processing

```{r}
#Setting up libraries and URLs

library(dplyr) #for data manipulation
library(stringr) #for string operations
library(readr) #reading data
```

```{r}
#Set up base URL for the UCI ML Repository and the dataset folder using string functions

uci.repo <-"http://archive.ics.uci.edu/ml/machine-learning-databases/"

dataset <- "audiology/audiology.standardized"
```

```{r}
# Using str_c, a function to concatenate strings
dataF <- str_c(uci.repo, dataset, ".data")
namesF <- str_c(uci.repo, dataset, ".names")
dataF
```

```{r}
# Read the data from the constructed URL into the 'data' dataframe
data <- read_csv(url(dataF), col_names = FALSE, na="?")
```

```{r}
#Finding the dimension of the data
dim(data)
```

```{r}
# Read the .names file line by line, storing each line as a 
# separate element in lines vector
lines <- read_lines(url(namesF))

lines |> head()
```

```{r}
# Extracts line 67 to 135 based on content observation and store in the names vector

names <- lines[67:135]
names
```

```{r}
#Based on observation that name line consists of name:valid values
#split on regular expression
names <- str_split_fixed(names, ":", 2) 
names
```

```{r}
# Take first column, consisting of names
names <- names[,1]
names
```

```{r}
# Clean-up names and trim spaces
names <-str_trim(names) |> str_replace_all("\\(|\\)", "") 
names
```

```{r}
# Put columns to the data

colnames(data)[1:69] <- names
data
```

```{r}
# Renaming the last two columns
colnames(data)[70:71] <- c("id", "class")
data
```

## Dealing with unknown values

```{r}
# Remove observation with many NAs

library(dplyr)

missing.value.rows <- data |>
  filter(!complete.cases(data))
missing.value.rows
```

```{r}
# Check how many NAs in each row
data <- data %>%
  mutate(na_count = rowSums(is.na(data)))
data
```

```{r}
# Check how many NAs in each column

data |>
  summarize(across(everything(), ~sum(is.na(.)), .names = "na_{.col}")) %>%
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count") %>%
  arrange(na_count)
```

```{r}
# Removing bser variable with 196 NAs
data.bser.removed <- data %>%
  select(-8) %>%
  summarise(across(everything(), ~sum(is.na(.)), .names = "na_{.col}"))
data.bser.removed
```

```{r}
#Using matches function to find index of a colname

data <- data %>%
  select(-matches("bser"))
```

### Dealing with mistaken characters

```{r}
mistaken <- c(2, 3, 4, "?")
class(mistaken)
```

```{r}
fixed <- parse_integer(mistaken, na = '?')
fixed
```

```{r}
class(fixed)
```

### Filling unknowns with most frequent values

```{r}
# Installing and loading necessary libraries
if(!require('DMwR2'))
  install.packages("DMwR2")
library(DMwR2)

data(algae, package = "DMwR2")
algae[48,]
```

```{r}
# Installing and loading necessary libraries
if(!require('car'))
   install.packages("car")
library(car)

# plot a QQ plot of mxPH
ggplot(algae, aes(sample = mxPH)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of mxPH") 
```

```{r}
#Using mean to fill the unknown
algae <- algae |>
  mutate(mxPH = ifelse(row_number() == 48, mean(mxPH, na.rm = TRUE), mxPH))
algae
```

```{r}
# Exploring the attributes 'Chla'
# Normal Q-Q Plot of Chla
ggplot(algae, aes(sample = Chla)) +
  geom_qq_band() +
  stat_qq_point() +
    stat_qq_line(color = "red", method = "identity", intercept = -2, slope = 1) +  
  ggtitle("Normal QQ plot of Chla") 
```

```{r}
#Finding the mean and median
mean(algae$Chla, na.rm = TRUE)
median(algae$Chla, na.rm = TRUE)
```

```{r}
# Using median to fill all missing values in Chla
algae <- algae |>
  mutate(Chla = if_else(is.na(Chla), median(Chla, na.rm = TRUE), Chla))
```

### Filling unknowns using linear regression

```{r}
# Plotting the correlations between variables
algae_numeric <- algae[, 4:18] %>%
  drop_na()  # Removes rows with NA values

cor_matrix <- algae_numeric |> correlate() |> plot()
```

```{r}
cor_matrix
```

```{r}
# Finding a linear model between P04 and oP04

algae <- algae %>%
  filter(rowSums(is.na(.)) / ncol(.) < 0.2)

m = lm(PO4 ~ oPO4, data = algae)
lm(formula = PO4 ~ oPO4, data = algae)
```

```{r}
# Summarizing
m |> 
  summary()
```

```{r}
m |> 
  summary() |> 
  tidy()    #creates a more readable output for linear regressions
```

```{r}
algae$PO4
```

```{r}
# Value filled with predicated value using the model
algae <- algae %>%
  mutate(PO4 = ifelse(row_number() == 28, 42.897 + 1.293 * oPO4, PO4))

res = resid(m)

oPO4_reduced <- algae %>%
  filter(row_number() != 28) %>%
  pull(oPO4)
```

```{r}
#Plotting the residual plot
ggplot(data = data.frame(oPO4 = m$model$oPO4, res = res), aes(x = oPO4, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    x = "oPO4",
    y = "residuals",
    title = "Residual Plot"
  )
```

```{r}
# CreatE a function fillP04
fillPO4 <- function(x) {
  if_else(is.na(x), 42.897 + 1.293 * x, x)
}
```

```{r}
#sapply() function is used to apply prediction to all observations with missing P04 values
algae[is.na(algae$PO4), "PO4"] <- sapply(algae[is.na(algae$PO4), "oPO4"], fillPO4)
```

### Filling unknowns by exploring similarities

```{r}
data(algae, package="DMwR2")
# Remove rows with NA values
algae <- algae[-manyNAs(algae), ] 
```

```{r}
# Using KNN Imputation to deal with unknowsn
algae <- knnImputation(algae, k = 10) 


data(algae, package="DMwR2") #get data again so there are unknown values
algae <- algae[-manyNAs(algae), ] 
algae <- knnImputation(algae, k = 10, meth="median") #use the median of 10 most similar samples
```

```{r}
#Display the internal code of KNN Imputation function
getAnywhere(knnImputation())
```

# Scaling and Normalization

```{r}
#Load necessary libraries and dataset
library(dplyr)
library(palmerpenguins)
data(penguins)
```

```{r}
# select and normalize only numeric columns from penguins dataset
penguins_numeric <- select(penguins, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

penguins_norm <- scale(penguins_numeric)

peng.norm <- cbind(as.data.frame(penguins_norm), species = penguins$species)
```

```{r}
summary(penguins)
```

```{r}
# Compute max and min values for numeric columns
max <- apply(select(penguins, -species), 2, max, na.rm=TRUE)
min <- apply(select(penguins, -species), 2, min, na.rm=TRUE)
```

```{r}
max
```

```{r}
min
```

```{r}
# Perfrom min-max normalization on numeric columns
penguin_scaled <- as.data.frame(lapply(penguins_numeric, function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))))

penguin_scaled <- cbind(penguins_norm, species = penguins$species)

summary(penguin_scaled)
```

## Discretizing variables(binning)

```{r}
# Load the Boston housing dataset from the MASS package
data(Boston, package="MASS")
summary(Boston$age)
```

### Equal-width Binning

```{r}
# Discretize the 'age' column into 5 equal-width bins
Boston$newAge <- dlookr::binning(Boston$age, 5, type = "equal") 
summary(Boston$newAge)
```

```{r}
# Discretize the 'age' column into 5 equal-width bins with specific labels

Boston$newAge <- dlookr::binning(Boston$age, nbins = 5, labels = c("very-young", "young", "mid", "older", "very-old"), type = "equal")

summary(Boston$newAge)
```

### Equal-depth Binning

```{r}
if(!require('Hmisc'))
  install.packages("Hmisc")
library(Hmisc)
Boston$newAge <- cut2(Boston$age, g = 5) #create 5 equal-depth bins

table(Boston$newAge)
```

```{r}
# Assign descriptive labels to the 5 equal-depth binning
Boston$newAge <- factor(cut2(Boston$age, g = 5), labels = c("very-young", "young", "mid", "older", "very-old"))

table(Boston$newAge)
```

```{r}
# Plot an equal-width histogram of width 10

hist(Boston$age, breaks = seq(0, 101,by = 10)) #seq() gives the function for breaks. The age ranges from 0 – 101.
```

```{r}
# Plot using ggplot2
library(ggplot2)

Boston |>
  ggplot(aes(x = age)) +
  geom_histogram(binwidth = 10)
```

## Decimal Scaling

```{r}
data <- c(10, 20, 30, 50, 100)
```

```{r}
#nchar counts the number of characters
(nDigits = nchar(max(abs(data)))) 

(decimalScale = data / (10^nDigits))
```

## Smoothing by bin mean

```{r}
age = c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30)

# Separate the data into bins of depth '3'
(bins = matrix(age, nrow = length(age) / 5, byrow = TRUE))
```

```{r}
# Find the average of each bin

(bin_means = apply(bins, 1, FUN = mean))
```

```{r}
# Replace values with their bin mean:

for (i in 1:nrow(bins)) {
   bins[i,] = bin_means[i]
 }
bins
```

```{r}
(age_bin_mean_smoothed = round(as.vector(t(bins)), 2))
```

# Variable correlations and dimensionality reduction

## Chi-squared test

```{r}
# Set up the contingency table for the prisoner's and victim's race
racetable = rbind(c(151,9), c(63,103))

# Perform the chi-squared test without continuity correction
test1 = chisq.test(racetable, correct=F)
test1
```

## Loglinear model

```{r}
# # Create a 2x2x2x2 data array for high school seniors' substance use and age

seniors <- array(data = c(911, 44, 538, 456, 3, 2, 43, 279, 911, 44, 538, 456, 3, 2, 43, 279), 
                  dim = c(2, 2, 2, 2),
                  dimnames = list("cigarette" = c("yes", "no"),
                                  "marijuana" = c("yes", "no"),
                                  "alcohol" = c("yes", "no"), 
                                  "age" =c("younger", "older")))
```

```{r}
# Observe how data is saved in the 2x2x2x2 array

seniors
```

```{r}
# Convert the multi-dimensional array to a table
seniors.tb <- as.table(seniors)
seniors.tb
```

```{r}
# Convert the table to a data frame for modeling
seniors.df <- as.data.frame(seniors.tb)
seniors.df
```

```{r}
# Fit a saturated loglinear model with all interactions

mod.S4 <- glm(Freq ~ (cigarette * marijuana * alcohol * age), data = seniors.df, family=poisson)
summary(mod.S4)
```

```{r}
# Remove the 'age' variable and fit another loglinear model

mod.S3 <- glm(Freq ~ (cigarette * marijuana * alcohol), data = seniors.df, family = poisson)
summary(mod.S3)
```

```{r}
mod.3 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, data = seniors.df, family = poisson)
summary(mod.3)
```

```{r}
# Compare and fit observed values
cbind(mod.3$data, fitted(mod.3))
```

## Correlations

```{r}
# Data manipulation
library(tidyr) 
penguins_numeric |> 
  drop_na() |>
  correlate()
```

## Principal components analysis (PCA)

```{r}
pca.data <- penguins |>
  drop_na() |>
  select(-species, -island, -sex) 

pca <- princomp(pca.data)
loadings(pca)
```

```{r}
# Pca result is a list, and the component scores are elements in the list
head(pca$scores) 
```

```{r}
# Reduce original data to the first three components
penguins_na <- penguins |> 
  drop_na()

peng.reduced <- data.frame(pca$scores[,1:3], Species = penguins_na$species)

head(peng.reduced)
```

```{r}
# For further analysis
if(!require('wavelets'))
  install.packages("wavelets")
library(wavelets)
```

```{r}
x <- c(2, 2, 0, 2, 3, 5, 4, 4)
wt <- dwt(x,filter="haar", n.levels = 3)
wt
```

```{r}
# Reconstruct the original
idwt(wt)
```

```{r}
# Obtain transform results using a different filter
xt = dwt(x, filter = wt.filter(c(0.5, -0.5)), n.levels = 3)
xt
```

```{r}
# Reconstructing the original

idwt(xt)
```

# Sampling

```{r}
# Setting seed for reproducibility
set.seed(1) 
# Creating a vector of ages
age <- c(25, 25, 25, 30, 33, 33, 35, 40, 45, 46, 52, 70)
```

## Simple random sampling, without replacement

```{r}
sample(age, 5)
```

## Simple random sampling, with replacement:

```{r}
sample(age, 5, replace = TRUE)
```

## Stratified sampling

```{r}
library(dplyr)
set.seed(1) # Setting seed for reproducibility
summary(algae) # Display summary of the algae dataset
```

```{r}
# Stratified sampling, taking 25% from each season

sample <-algae |> group_by(season) |> sample_frac(0.25)
summary(sample)
```

## Cluster sampling

```{r}
if(!require('sampling'))
  install.packages("sampling")

library(sampling)

# Updated age data for cluster sampling
age <- c(13, 15, 16, 16, 19, 20, 20, 21, 22, 22, 25, 25, 25, 25, 30, 33, 33, 35, 35, 35, 35, 36, 40, 45, 46, 52, 70)

# Perform kmeans clustering on age to form 3 clusters
s <- kmeans(age, 3) #cluster on age to form 3 clusters
s$cluster
```

```{r}
ageframe <- data.frame(age)

# add cluster label as a new column
ageframe$condition <- s$cluster 
cluster(ageframe, clustername = "condition", size = 2) # select 2 clusters out of the three
```

# Handling Text Datasets

```{r}
# Load necessary libraries
pacman::p_load(tm,
               SnowballC)
# read corpus
data <- read.csv(here::here("data", "Emails.csv"), stringsAsFactors = FALSE)

docs <- Corpus(VectorSource(data$RawText))
mode(docs)
```

```{r}
# Inspect a document

docs[[20]]
```

```{r}
# Preprocess the text in the corpus

docs <- docs |>
         tm_map(removePunctuation) |>
         tm_map(content_transformer(tolower)) |> #to lower case
         tm_map(removeNumbers) |>
         tm_map(removeWords, stopwords("en")) |> #stopwords, such as a, an.
         tm_map(stripWhitespace) |>
         tm_map(stemDocument) #e.g. computer -> comput
```

```{r}
# Display the content of a document after preprocessing

content(docs[[20]])
```

```{r}
# Convert the text to a document-term matrix using TF*IDF scores

DTData <- DocumentTermMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
# Display the document-term matrix

DTData
```

```{r}
# Inspect a portion of the document-term matrix

inspect(DTData[1:2, 1:5])
```

```{r}
# Create a term-document matrix (inverted index)

TDData <- TermDocumentMatrix(docs, control = list(weighting = weightTfIdf))
```

```{r}
inspect(TDData[1:2, 1:5])
```

# Explore the dataset

```{r}
# Find frequent terms in the term-document matrix
findFreqTerms(TDData, lowfreq = 75, highfreq = 1000)

# Find associations among terms in the term-document matrix
findAssocs(TDData, terms = "bill", corlimit = 0.25)

# Find associations among terms in the document-term matrix
findAssocs(DTData, terms=c("bill"), corlimit = 0.25)

# Find associations among terms in the document-term matrix for the term "schedul"
findAssocs(DTData, terms=c("schedul"), corlimit = 0.3)
```

# Word Cloud

```{r}
if(!require('wordcloud'))
  install.packages("wordcloud")
if(!require('RColorBrewer'))
  install.packages("RColorBrewer")
library(wordcloud)
```

```{r}
# Loading data
data <- as.matrix(TDData)
freq <- sort(rowSums(data), decreasing = TRUE)
base <-data.frame(word = names(freq), freq = freq)
```

```{r}
# png() opens a new device ‘png’ to output the graph to a local file
png(file = "wordCloud.png", width = 1000, height = 700, bg= "grey30")

wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

```{r}
#Output the graph
wordcloud(base$word, base$freq, col = terrain.colors(length(base$word), alpha = 0.9), 
random.order = FALSE, rot.per = 0.3, scale = c(1, .1))
```

# One Hot Encoding

```{r}

# One hot encoding
if(!require('onehot'))
  install.packages("onehot")
library(onehot)
d <- data.frame(language=c("javascript", "python", "java"), hours=c(10, 3, 5) )
d$language <- as.factor(d$language) #convert the column to be encoded to Factor
encoded <- onehot(d)
new_d <- predict(encoded, d)
new_d
```

```{r}
# One hot encoding for data frame with multi-value cells 
if(!require('qdapTools'))
  install.packages("qdapTools")
library(qdapTools)
d <- data.frame(language=c("javascript, python", "java"), hours = c(3, 5) )
d
```

```{r}
dlist <- as.list(d)
new_d <- data.frame(cbind(dlist, mtabulate(strsplit(as.character(dlist$language), ", ")))) 

new_d
```
